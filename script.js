// API endpoint now uses secure Netlify function
const API_URL = '/.netlify/functions/generate-config';

// --- DOM Elements ---
const form = document.getElementById('mig-form');
const inferenceInput = document.getElementById('inference-jobs');
const trainingInput = document.getElementById('training-jobs');
const memoryRequirement = document.getElementById('memory-requirement');
const environmentSelect = document.getElementById('environment');
const loadingDiv = document.getElementById('loading');
const resultsDiv = document.getElementById('results');
const errorDiv = document.getElementById('error');
const errorMessage = document.getElementById('error-message');
const explanationOutput = document.getElementById('explanation-output');
const commandsOutput = document.getElementById('commands-output');
const resourceBreakdown = document.getElementById('resource-breakdown');
const configBadge = document.getElementById('config-badge');
const copyButton = document.getElementById('copy-button');
const demoButton = document.getElementById('demo-btn');
const explainIssueButton = document.getElementById('explain-issue-btn');
const issueModal = document.getElementById('issue-modal');
const closeModal = document.getElementById('close-modal');
const analyzeIssueButton = document.getElementById('analyze-issue');
const issueDescription = document.getElementById('issue-description');
const tooltip = document.getElementById('tooltip');
const commandsWithExplanations = document.getElementById('commands-with-explanations');
const riskAnalysis = document.getElementById('risk-analysis');

// --- Event Listeners ---
form.addEventListener('submit', handleFormSubmit);
copyButton.addEventListener('click', copyCommandsToClipboard);
demoButton.addEventListener('click', handleDemoMode);
explainIssueButton.addEventListener('click', openIssueModal);
closeModal.addEventListener('click', closeIssueModal);
analyzeIssueButton.addEventListener('click', handleIssueAnalysis);
document.addEventListener('click', handleTooltips);
document.addEventListener('mouseover', showTooltip);
document.addEventListener('mouseout', hideTooltip);

// Close modal when clicking outside
issueModal.addEventListener('click', (e) => {
    if (e.target === issueModal) closeIssueModal();
});

// --- Functions ---

/**
 * Main handler for the form submission.
 * @param {Event} e The form submission event.
 */
async function handleFormSubmit(e) {
    e.preventDefault();

    // Hide all states and show loading
    hideAllStates();
    loadingDiv.classList.add('active');

    const config = getFormConfig();
    const masterPrompt = buildAdvancedPrompt(config);

    try {
        const response = await fetch(API_URL, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                prompt: masterPrompt,
                type: 'configuration'
            })
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error || `API Error: ${response.status}`);
        }

        const data = await response.json();
        const responseText = data.choices[0].message.content.trim();
        
        try {
            const parsedResult = JSON.parse(responseText);
            displayResults(parsedResult, config);
        } catch (parseError) {
            // If JSON parsing fails, try to extract JSON from the response
            const jsonMatch = responseText.match(/\{[\s\S]*\}/);
            if (jsonMatch) {
                const parsedResult = JSON.parse(jsonMatch[0]);
                displayResults(parsedResult, config);
            } else {
                throw new Error("AI returned invalid JSON format");
            }
        }

    } catch (err) {
        displayError(`Configuration Error: ${err.message}`);
    } finally {
        loadingDiv.classList.remove('active');
    }
}

/**
 * Handle demo mode for showcasing without API key
 */
async function handleDemoMode() {
    hideAllStates();
    loadingDiv.classList.add('active');
    
    const config = getFormConfig();
    
    // Simulate API delay for realistic demo
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    const demoData = {
        "strategy": `For this ${config.environment} workload requiring ${config.inferenceJobs} inference endpoints and ${config.trainingJobs} training pipeline(s), I'm implementing a balanced MIG configuration that optimizes both isolation and resource utilization. The strategy creates ${config.inferenceJobs}x 1g.10gb instances specifically for lightweight inference tasks (ideal for 7B-13B parameter models), providing maximum isolation while ensuring low latency. Additionally, I'm allocating ${config.trainingJobs}x 3g.40gb instance(s) for training pipelines, which provides sufficient memory and compute for fine-tuning operations with reasonable batch sizes. This configuration maintains optimal price-performance ratio for ${config.environment} workloads while ensuring proper resource isolation.`,
        
        "commands": `# Disable existing MIG configuration\nsudo nvidia-smi -mig 0\n\n# Enable MIG mode\nsudo nvidia-smi -mig 1\n\n# Create GPU instances with optimal profile distribution\nsudo nvidia-smi mig -cgi ${Array(config.inferenceJobs).fill('1g.10gb').concat(Array(config.trainingJobs).fill('3g.40gb')).join(',')} -C\n\n# Verify the configuration\nsudo nvidia-smi mig -lgip\n\n# List compute instances\nsudo nvidia-smi mig -lcip`,
        
        "resources": {
            "memory_total": `${(config.inferenceJobs * 10) + (config.trainingJobs * 40)}GB`,
            "compute_units": `${config.inferenceJobs + (config.trainingJobs * 3)}`,
            "efficiency_score": "88%"
        },
        
        "warnings": [
            "This is a demo configuration - actual production deployments may require additional tuning",
            "Monitor memory usage during peak inference loads",
            "Consider implementing proper monitoring and alerting for GPU utilization metrics"
        ]
    };
    
    displayResults(demoData, config);
    loadingDiv.classList.remove('active');
}

// Tooltip data
const tooltipData = {
    inference: {
        title: "Inference Endpoints",
        content: `
            <p><strong>What it controls:</strong> Number of separate AI model serving instances</p>
            <p><strong>Real-world examples:</strong></p>
            <ul>
                <li>Customer A's chatbot (needs isolation from Customer B)</li>
                <li>Different model versions for A/B testing</li>
                <li>Separate services (recommendations, search, classification)</li>
            </ul>
            <p><strong>Why it matters:</strong> Each endpoint gets dedicated GPU memory, preventing one model from crashing others.</p>
        `
    },
    training: {
        title: "Training Pipelines", 
        content: `
            <p><strong>What it controls:</strong> Number of simultaneous training/fine-tuning jobs</p>
            <p><strong>Real-world examples:</strong></p>
            <ul>
                <li>Team A fine-tuning for legal documents</li>
                <li>Team B training multimodal models</li>
                <li>Continuous learning pipelines</li>
            </ul>
            <p><strong>Why it matters:</strong> Training needs 3-5x more memory than inference and runs for hours/days.</p>
        `
    },
    memory: {
        title: "Memory Strategy",
        content: `
            <p><strong>What it controls:</strong> How GPU memory is allocated across workloads</p>
            <p><strong>Strategies:</strong></p>
            <ul>
                <li><strong>Conservative:</strong> More, smaller instances (max isolation)</li>
                <li><strong>Balanced:</strong> Mixed sizes (flexibility + efficiency)</li>
                <li><strong>Aggressive:</strong> Fewer, larger instances (max memory per workload)</li>
            </ul>
            <p><strong>Business impact:</strong> Affects cost efficiency and performance characteristics.</p>
        `
    },
    environment: {
        title: "Environment Type",
        content: `
            <p><strong>What it controls:</strong> Optimization target for the configuration</p>
            <p><strong>Options:</strong></p>
            <ul>
                <li><strong>Production:</strong> Reliability, performance, fault tolerance</li>
                <li><strong>Development:</strong> Flexibility, fast iteration, resource sharing</li>
                <li><strong>Research:</strong> Maximum compute, experimental features</li>
            </ul>
            <p><strong>Why it matters:</strong> Different environments have different priorities and constraints.</p>
        `
    }
};

/**
 * Show tooltip on hover
 */
function showTooltip(e) {
    const trigger = e.target.closest('.tooltip-trigger');
    if (!trigger) return;
    
    const type = trigger.dataset.tooltip;
    const data = tooltipData[type];
    if (!data) return;
    
    const tooltipContent = tooltip.querySelector('.tooltip-content') || document.createElement('div');
    tooltipContent.className = 'tooltip-content';
    tooltipContent.innerHTML = `<h4>${data.title}</h4>${data.content}`;
    tooltip.innerHTML = '';
    tooltip.appendChild(tooltipContent);
    
    // Position tooltip
    const rect = trigger.getBoundingClientRect();
    tooltip.style.left = rect.left + 'px';
    tooltip.style.top = (rect.bottom + 10) + 'px';
    tooltip.classList.remove('hidden');
}

/**
 * Hide tooltip
 */
function hideTooltip(e) {
    const trigger = e.target.closest('.tooltip-trigger');
    if (!trigger) {
        tooltip.classList.add('hidden');
    }
}

/**
 * Handle tooltip clicks
 */
function handleTooltips(e) {
    if (!e.target.closest('.tooltip-trigger')) {
        tooltip.classList.add('hidden');
    }
}

/**
 * Open issue explanation modal
 */
function openIssueModal() {
    issueModal.classList.remove('hidden');
    issueDescription.focus();
}

/**
 * Close issue explanation modal
 */
function closeIssueModal() {
    issueModal.classList.add('hidden');
    issueDescription.value = '';
}

/**
 * Analyze user issue and set form inputs
 */
async function handleIssueAnalysis() {
    const issue = issueDescription.value.trim();
    if (!issue) {
        alert('Please describe your issue first');
        return;
    }
    
    closeIssueModal();
    hideAllStates();
    loadingDiv.classList.add('active');
    
    try {
        const analysisPrompt = `
**INFRASTRUCTURE PROBLEM ANALYSIS**

User Issue: "${issue}"

You are a Senior DevOps Engineer. Analyze this user's GPU infrastructure challenge and determine the optimal configuration parameters.

Based on the description, determine:
1. How many inference endpoints they likely need (0-7)
2. How many training pipelines they need (0-7)  
3. What memory strategy fits: "auto", "10", "20", or "40"
4. What environment type: "production", "development", or "research"

Also provide:
- A strategic explanation of why this configuration solves their problem
- Shell commands for implementation
- Resource breakdown
- Risk analysis of what happens if they don't fix this

Respond with ONLY a JSON object:
{
  "config": {
    "inferenceJobs": 2,
    "trainingJobs": 1, 
    "memoryReq": "auto",
    "environment": "production"
  },
  "strategy": "Based on your description...",
  "commands": "sudo nvidia-smi -mig 0\\n...",
  "resources": {"memory_total": "60GB", "compute_units": "6", "efficiency_score": "88%"},
  "riskAnalysis": "Without this solution, you would face..."
}`;

        const response = await fetch(API_URL, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                prompt: analysisPrompt,
                type: 'analysis'
            })
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error || `API Error: ${response.status}`);
        }

        const data = await response.json();
        const responseText = data.choices[0].message.content.trim();
        const result = JSON.parse(responseText);
        
        // Set form inputs based on analysis
        inferenceInput.value = result.config.inferenceJobs;
        trainingInput.value = result.config.trainingJobs;
        memoryRequirement.value = result.config.memoryReq;
        environmentSelect.value = result.config.environment;
        
        // Generate enhanced analysis with dynamic components
        const resourceAllocation = generateResourceAllocation(result.config);
        const dynamicRiskAnalysis = generateRiskAnalysis(result.config);
        
        // Display results
        const enhancedResult = {
            strategy: result.strategy,
            commands: result.commands,
            resources: resourceAllocation,
            riskAnalysis: dynamicRiskAnalysis
        };
        
        displayResults(enhancedResult, result.config);
        
        // Scroll to results
        setTimeout(() => {
            resultsDiv.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }, 500);
        
    } catch (err) {
        displayError(`Analysis Error: ${err.message}`);
    } finally {
        loadingDiv.classList.remove('active');
    }
}

/**
 * Get current form configuration
 */
function getFormConfig() {
    return {
        inferenceJobs: parseInt(inferenceInput.value),
        trainingJobs: parseInt(trainingInput.value),
        memoryReq: memoryRequirement.value,
        environment: environmentSelect.value,
        totalJobs: parseInt(inferenceInput.value) + parseInt(trainingInput.value)
    };
}
                /**
 * Builds the advanced production-grade prompt for MIG configuration
 * This is the "secret sauce" - a comprehensive prompt for real-world scenarios
 */
function buildAdvancedPrompt(config) {
    return `
**PRODUCTION AI INFRASTRUCTURE CONFIGURATION REQUEST**

You are architecting a NVIDIA H100 80GB PCIe GPU MIG configuration for a production AI platform. This is a critical infrastructure decision that will impact:
- Model serving latency and throughput
- Training pipeline efficiency  
- Resource utilization and cost optimization
- Isolation and fault tolerance

**CURRENT WORKLOAD REQUIREMENTS:**
- Inference Endpoints: ${config.inferenceJobs} (concurrent model serving instances)
- Training Pipelines: ${config.trainingJobs} (fine-tuning and training workloads)
- Memory Strategy: ${config.memoryReq}
- Environment: ${config.environment}
- Total Isolation Requirements: ${config.totalJobs} separate GPU instances

**PRODUCTION CONSTRAINTS:**
1. H100 80GB supports max 7 MIG instances with profiles: 1g.10gb, 2g.20gb, 3g.40gb, 4g.40gb, 7g.80gb
2. Inference workloads typically need 10-20GB for most LLMs (7B-13B models)
3. Training/fine-tuning needs 20-40GB+ depending on model size and batch size
4. ${config.environment} environment requires ${config.environment === 'production' ? 'maximum reliability and performance' : config.environment === 'development' ? 'flexibility and resource efficiency' : 'experimental features and maximum compute'}

**YOUR EXPERT ANALYSIS MUST INCLUDE:**

1. **Strategic Decision**: Justify your MIG profile selection based on:
   - Memory efficiency vs isolation requirements
   - Workload characteristics and resource needs
   - Environment-specific optimizations
   - Potential bottlenecks and mitigation strategies

2. **Complete Implementation**: Provide the exact shell commands for:
   - Clearing existing MIG configuration safely
   - Enabling MIG mode with proper validation
   - Creating the optimal instance configuration
   - Verification and health check commands

3. **Resource Breakdown**: Calculate and explain:
   - Total memory allocation across instances
   - Compute unit distribution
   - Expected performance characteristics
   - Scalability considerations

**OUTPUT FORMAT:**
Respond with ONLY a valid JSON object containing these keys:
- "strategy": Detailed explanation of your configuration decisions and rationale
- "commands": Complete shell command sequence (newline separated)
- "resources": Object with memory_total, compute_units, and efficiency_score
- "warnings": Array of potential issues or recommendations

**EXAMPLE STRUCTURE:**
{
  "strategy": "For this production workload, I'm configuring 2x 1g.10gb instances for lightweight inference serving and 1x 3g.40gb instance for training. This provides optimal isolation while maximizing resource utilization...",
  "commands": "sudo nvidia-smi -mig 0
sudo nvidia-smi -mig 1
sudo nvidia-smi mig -cgi 1g.10gb,1g.10gb,3g.40gb -C
sudo nvidia-smi mig -gi",
  "resources": {"memory_total": "60GB", "compute_units": "6", "efficiency_score": "85%"},
  "warnings": ["Consider monitoring memory usage during peak loads", "Training workload may benefit from larger batch sizes"]
}
`;
}

/**
 * Display comprehensive results with enhanced visualizations
 */
function displayResults(data, config) {
    if (!data.strategy || !data.commands) {
        displayError("Invalid configuration returned. Please try again.");
        return;
    }

    // Set configuration badge
    const totalMemory = data.resources?.memory_total || `${config.totalJobs * 20}GB`;
    configBadge.textContent = `${config.totalJobs} Instances ‚Ä¢ ${totalMemory}`;
    configBadge.className = 'config-badge';

    // Display strategy explanation
    explanationOutput.textContent = data.strategy;

    // Display commands with explanations
    createCommandExplanations(data.commands);

    // Create resource breakdown visualization
    createResourceBreakdown(data.resources, config);

    // Create risk analysis
    createRiskAnalysis(data.riskAnalysis || generateRiskAnalysis(config));

    // Show results
    hideAllStates();
    resultsDiv.classList.add('active');
    
    // Auto-scroll to results
    setTimeout(() => {
        resultsDiv.scrollIntoView({ behavior: 'smooth', block: 'start' });
    }, 300);
}

/**
 * Create command explanations with detailed breakdown
 */
function createCommandExplanations(commands) {
    const commandList = commands.split('\n').filter(cmd => cmd.trim());
    
    const commandExplanations = {
        'sudo nvidia-smi -mig 0': {
            title: 'üîß Disable MIG Mode',
            description: 'Safely disables Multi-Instance GPU mode and clears any existing partitions. This ensures a clean slate for our new configuration.'
        },
        'sudo nvidia-smi -mig 1': {
            title: 'üöÄ Enable MIG Mode', 
            description: 'Activates Multi-Instance GPU functionality, allowing the H100 to be partitioned into multiple isolated instances.'
        },
        'sudo nvidia-smi mig -cgi': {
            title: '‚öôÔ∏è Create GPU Instances',
            description: 'Creates the actual MIG instances with specified memory and compute allocations. Each instance becomes an independent GPU resource.'
        },
        'sudo nvidia-smi mig -lgip': {
            title: 'üìã List GPU Instances',
            description: 'Verifies that all instances were created successfully and displays their unique identifiers and resource allocations.'
        },
        'sudo nvidia-smi mig -lcip': {
            title: '‚úÖ List Compute Instances', 
            description: 'Shows the compute instances that can be used by applications, confirming the configuration is ready for workloads.'
        }
    };

    let html = '<div class="command-explanation">';
    
    commandList.forEach(cmd => {
        const cleanCmd = cmd.trim().replace(/^#.*/, '').trim();
        if (!cleanCmd) return;
        
        let explanation = null;
        for (const [pattern, data] of Object.entries(commandExplanations)) {
            if (cleanCmd.includes(pattern.replace('sudo nvidia-smi ', ''))) {
                explanation = data;
                break;
            }
        }
        
        if (cmd.startsWith('#')) {
            html += `<div class="command-item">
                <div class="command-title">üìù ${cmd.replace('#', '').trim()}</div>
            </div>`;
        } else if (explanation) {
            html += `<div class="command-item">
                <div class="command-title">${explanation.title}</div>
                <div class="command-code">${cleanCmd}</div>
                <div class="command-description">${explanation.description}</div>
            </div>`;
        } else {
            html += `<div class="command-item">
                <div class="command-code">${cleanCmd}</div>
            </div>`;
        }
    });
    
    html += '</div>';
    html += `<button id="copy-commands" class="copy-btn" style="margin-top: 1rem;">Copy All Commands</button>`;
    
    commandsWithExplanations.innerHTML = html;
    
    // Add copy functionality
    document.getElementById('copy-commands').addEventListener('click', () => {
        copyCommandsToClipboard(commands);
    });
}

/**
 * Create risk analysis section
 */
function createRiskAnalysis(riskContent) {
    // Handle new dynamic risk analysis format
    if (riskContent && typeof riskContent === 'object' && riskContent.scenarios) {
        const scenarios = riskContent.scenarios || [];
        const metrics = riskContent.metrics || [];
        
        const scenariosHtml = scenarios.map(scenario => `
            <div class="risk-scenario">
                <p>${scenario}</p>
            </div>
        `).join('');
        
        const metricsHtml = metrics.map(metric => `
            <div class="risk-item">
                <strong>${metric.label}:</strong> ${metric.value}
            </div>
        `).join('');
        
        riskAnalysis.innerHTML = `
            <div class="risk-story">
                <h4>üî• Real-World Impact Analysis</h4>
                ${scenariosHtml}
            </div>
            <div class="risk-metrics">
                <h4>üìä Risk Metrics</h4>
                ${metricsHtml}
            </div>
        `;
    } else {
        // Fallback for old format or missing data
        const fallbackContent = riskContent || "Without proper MIG configuration, you risk resource conflicts, degraded performance, and operational inefficiencies that could cost thousands in downtime and wasted compute resources.";
        
        riskAnalysis.innerHTML = `
            <div class="risk-story">
                <h4>üî• The Cost of Inaction</h4>
                <p>${fallbackContent}</p>
            </div>
            <div class="risk-metrics">
                <div class="risk-item">
                    <strong>Potential Downtime:</strong> 4-8 hours during resource conflicts
                </div>
                <div class="risk-item">
                    <strong>Performance Impact:</strong> 40-70% throughput reduction
                </div>
                <div class="risk-item">
                    <strong>Cost per Hour:</strong> $2,000-5,000 in lost productivity
                </div>
            </div>
        `;
    }
}

/**
 * Generate comprehensive resource allocation breakdown
 */
function generateResourceAllocation(config) {
    const totalMemory = 80; // H100 80GB
    const totalComputeUnits = 7; // H100 MIG instances
    
    // Calculate memory per service type
    const inferenceMemoryPerJob = config.inferenceJobs > 0 ? 
        Math.floor((totalMemory * 0.6) / config.inferenceJobs) : 0;
    const trainingMemoryPerJob = config.trainingJobs > 0 ? 
        Math.floor((totalMemory * 0.35) / config.trainingJobs) : 0;
    
    // Calculate compute allocation
    const inferenceComputeUnits = Math.min(config.inferenceJobs, Math.floor(totalComputeUnits * 0.6));
    const trainingComputeUnits = Math.min(config.trainingJobs, Math.floor(totalComputeUnits * 0.4));
    
    // Performance estimates
    const inferenceTokensPerSecond = inferenceMemoryPerJob > 20 ? 150 : inferenceMemoryPerJob > 10 ? 80 : 40;
    const trainingBatchSize = trainingMemoryPerJob > 30 ? 32 : trainingMemoryPerJob > 15 ? 16 : 8;
    
    // Cost analysis
    const hourlyGPUCost = 3.20; // Estimated H100 cost per hour
    const utilizationRate = config.inferenceJobs + config.trainingJobs > 4 ? 0.95 : 0.75;
    const effectiveHourlyCost = hourlyGPUCost * utilizationRate;
    
    // Generate instance specifications
    const instances = [];
    
    // Inference instances
    for (let i = 0; i < config.inferenceJobs; i++) {
        instances.push({
            type: 'Inference',
            name: `inference-service-${i + 1}`,
            memory: `${inferenceMemoryPerJob}GB`,
            computeUnits: Math.ceil(inferenceComputeUnits / config.inferenceJobs),
            performance: `~${inferenceTokensPerSecond} tokens/sec`,
            isolation: 'Complete memory isolation',
            priority: 'High',
            healthCheck: 'GPU memory < 90%',
            autoScale: 'Horizontal scaling enabled'
        });
    }
    
    // Training instances  
    for (let i = 0; i < config.trainingJobs; i++) {
        instances.push({
            type: 'Training',
            name: `training-pipeline-${i + 1}`,
            memory: `${trainingMemoryPerJob}GB`,
            computeUnits: Math.ceil(trainingComputeUnits / config.trainingJobs),
            performance: `Batch size: ${trainingBatchSize}`,
            isolation: 'Isolated from inference',
            priority: 'Medium',
            healthCheck: 'Training progress monitoring',
            autoScale: 'Vertical scaling for large datasets'
        });
    }
    
    // System overhead
    const systemOverhead = {
        memory: `${totalMemory - (inferenceMemoryPerJob * config.inferenceJobs) - (trainingMemoryPerJob * config.trainingJobs)}GB`,
        purpose: 'OS, drivers, monitoring, failover buffer',
        monitoring: 'nvidia-smi, DCGM metrics',
        logging: 'Centralized logging with retention'
    };
    
    return {
        summary: {
            total_memory: `${totalMemory}GB`,
            allocated_memory: `${(inferenceMemoryPerJob * config.inferenceJobs) + (trainingMemoryPerJob * config.trainingJobs)}GB`,
            reserved_system: systemOverhead.memory,
            compute_units_used: `${inferenceComputeUnits + trainingComputeUnits}/${totalComputeUnits}`,
            estimated_utilization: `${Math.round(utilizationRate * 100)}%`,
            hourly_cost: `$${effectiveHourlyCost.toFixed(2)}`,
            monthly_cost: `$${(effectiveHourlyCost * 24 * 30).toLocaleString()}`
        },
        instances: instances,
        systemOverhead: systemOverhead,
        performance: {
            inference_latency: `${config.inferenceJobs > 2 ? '50-80ms' : '30-50ms'}`,
            training_throughput: `${config.trainingJobs > 1 ? '85-95%' : '95-98%'} efficiency`,
            memory_efficiency: `${Math.round((1 - (parseFloat(systemOverhead.memory) / totalMemory)) * 100)}%`,
            fault_tolerance: config.inferenceJobs > 1 ? 'High (redundant services)' : 'Medium (single point)',
            scaling_capability: 'Auto-scale within MIG boundaries'
        },
        monitoring: {
            gpu_metrics: 'Real-time GPU utilization, temperature, power',
            memory_tracking: 'Per-instance memory usage and leaks detection',
            performance_kpis: 'Latency, throughput, error rates',
            alerting: 'Slack/PagerDuty integration for threshold breaches',
            dashboards: 'Grafana with NVIDIA DCGM data source'
        },
        security: {
            isolation_level: 'Hardware-level MIG isolation',
            access_control: 'RBAC with Kubernetes namespaces',
            data_encryption: 'In-transit and at-rest encryption',
            audit_logging: 'All GPU operations logged and monitored',
            compliance: config.environment === 'production' ? 'SOC2, GDPR ready' : 'Development standards'
        }
    };
}

/**
 * Generate dynamic risk analysis based on configuration
 */
function generateRiskAnalysis(config) {
    const scenarios = [];
    const metrics = [];
    
    // Analyze based on inference jobs
    if (config.inferenceJobs > 3) {
        scenarios.push(`üî• **Multi-Service Cascade Failure**: With ${config.inferenceJobs} inference services sharing unpartitioned GPU memory, imagine this Black Friday scenario: Your recommendation engine suddenly spikes to 35GB usage, starving your chatbot (needs 8GB) and fraud detection (needs 12GB). Result: Customer support crashes during peak traffic, fraud detection goes offline during highest-risk period. Last year, a similar retailer lost $2.3M in 4 hours from this exact scenario.`);
        metrics.push({
            label: "Service Downtime Risk",
            value: `${config.inferenceJobs}x cascading failures possible`
        });
        metrics.push({
            label: "Peak Traffic Impact", 
            value: "$50,000-200,000 lost revenue/hour"
        });
    } else if (config.inferenceJobs > 1) {
        scenarios.push(`‚ö†Ô∏è **Resource Competition Crisis**: Your ${config.inferenceJobs} AI services are fighting for the same 80GB pool. When one service has a memory leak or processes an unusually large request, it steals resources from others. Real example: A startup's chatbot crashed their fraud detection during a security incident because both services competed for GPU memory - they lost a major client and faced regulatory scrutiny.`);
        metrics.push({
            label: "Memory Conflict Probability",
            value: "60-80% during peak loads"
        });
    }
    
    // Analyze based on training jobs
    if (config.trainingJobs > 0 && config.inferenceJobs > 0) {
        scenarios.push(`üö® **Training vs Production War**: Your ${config.trainingJobs} training pipeline(s) will compete directly with ${config.inferenceJobs} customer-facing services. Real scenario from last month: A fintech company's model training consumed 70GB during market hours, causing their real-time fraud detection to fail. Result: $500K in fraudulent transactions went undetected, regulatory fines, and 6 months of customer trust rebuilding.`);
        metrics.push({
            label: "Production Interference",
            value: "Training blocks inference 40-70% of time"
        });
        metrics.push({
            label: "Training Efficiency Loss",
            value: "2-3x longer completion times"
        });
    } else if (config.trainingJobs > 1) {
        scenarios.push(`‚öôÔ∏è **Research Bottleneck**: ${config.trainingJobs} teams competing for training resources means experiments queue up, innovation slows down, and data scientists spend more time waiting than researching. Your AI competitive advantage erodes as competitors ship features faster.`);
        metrics.push({
            label: "Research Productivity",
            value: `${config.trainingJobs} teams blocking each other`
        });
    }
    
    // Environment-specific risks
    if (config.environment === 'production') {
        scenarios.push(`üíº **Enterprise SLA Breach**: In production environments, resource conflicts trigger SLA violations. Enterprise customers expect 99.9% uptime. Each incident: $10K penalty + customer trust damage + competitive disadvantage. Three incidents = contract cancellation.`);
        metrics.push({
            label: "SLA Penalty Risk",
            value: "$10,000-50,000 per incident"
        });
        metrics.push({
            label: "Customer Churn Risk",
            value: "15-30% after repeated failures"
        });
    }
    
    // Memory strategy risks
    if (config.memoryReq === 'auto' || !config.memoryReq) {
        scenarios.push(`üé≤ **Configuration Gambling**: Without deliberate memory strategy, you're essentially gambling with $40,000 hardware. Suboptimal allocation = wasted capacity = burning money. Proper partitioning typically improves utilization by 60-150%.`);
        metrics.push({
            label: "Resource Waste",
            value: "40-70% of $40K GPU capacity unused"
        });
    }
    
    // Cost analysis
    const hourlyCost = calculateHourlyCost(config);
    metrics.push({
        label: "Estimated Hourly Loss",
        value: `$${hourlyCost.toLocaleString()}/hour during conflicts`
    });
    
    return {
        scenarios: scenarios.slice(0, 2), // Limit to most relevant scenarios
        metrics: metrics
    };
}

function calculateHourlyCost(config) {
    let baseCost = 1000; // Base cost per hour for GPU infrastructure issues
    
    // Scale by number of services at risk
    baseCost += (config.inferenceJobs * 500);
    baseCost += (config.trainingJobs * 300);
    
    // Environment multiplier
    const envMultiplier = {
        'production': 3,
        'development': 1.5,
        'research': 2
    };
    
    baseCost *= envMultiplier[config.environment] || 2;
    
    return Math.min(baseCost, 15000); // Cap at reasonable maximum
}

/**
 * Create visual resource breakdown
 */
function createResourceBreakdown(resources, config) {
    const memoryTotal = resources?.memory_total || 'Unknown';
    const computeUnits = resources?.compute_units || 'Unknown';
    const efficiency = resources?.efficiency_score || 'N/A';

    resourceBreakdown.innerHTML = `
        <div class="resource-item">
            <div class="resource-label">Total Memory Allocated</div>
            <div class="resource-value">${memoryTotal}</div>
        </div>
        <div class="resource-item">
            <div class="resource-label">Compute Units</div>
            <div class="resource-value">${computeUnits}</div>
        </div>
        <div class="resource-item">
            <div class="resource-label">Efficiency Score</div>
            <div class="resource-value">${efficiency}</div>
        </div>
        <div class="resource-item">
            <div class="resource-label">Instance Distribution</div>
            <div class="resource-value">${config.inferenceJobs} Inference + ${config.trainingJobs} Training</div>
        </div>
    `;
}

/**
 * Hide all UI states
 */
function hideAllStates() {
    loadingDiv.classList.remove('active');
    errorDiv.classList.remove('active');
    resultsDiv.classList.remove('active');
}

/**
 * Display error with enhanced messaging
 */
function displayError(message) {
    hideAllStates();
    errorMessage.textContent = message;
    errorDiv.classList.add('active');
}

/**
 * Enhanced clipboard functionality
 */
async function copyCommandsToClipboard(commandText = null) {
    const textToCopy = commandText || document.getElementById('copy-commands')?.previousElementSibling?.textContent || '';
    
    if (!navigator.clipboard) {
        // Fallback for older browsers
        const textArea = document.createElement('textarea');
        textArea.value = textToCopy;
        document.body.appendChild(textArea);
        textArea.select();
        document.execCommand('copy');
        document.body.removeChild(textArea);
    } else {
        await navigator.clipboard.writeText(textToCopy);
    }
    
    const button = document.getElementById('copy-commands') || copyButton;
    const originalText = button.textContent;
    button.textContent = '‚úì Copied';
    button.style.background = 'var(--success)';
    
    setTimeout(() => {
        button.textContent = originalText;
        button.style.background = '';
    }, 2000);
}


